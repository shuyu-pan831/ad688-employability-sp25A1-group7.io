---
title: "Data Analysis"
subtitle: "Comprehensive Data Cleaning & Exploratory Analysis of Job Market Trends"
author:
  - name: Guowei Yan
  - name: Jiaxin Wang
  - name: Shuyu Pan
  - name: Yixuan Chen
bibliography: /home/ubuntu/ad688-employability-sp25A1-group7.io/references.bib
csl: /home/ubuntu/ad688-employability-sp25A1-group7.io/csl/econometrica.csl
format: 
  html:
    toc: true
    number-sections: true
    df-print: paged
---

```{python}
import pandas as pd
import missingno as msno
import matplotlib.pyplot as plt
import warnings
```


```{python}
lightcast_data = pd.read_csv("/home/ubuntu/lightcast_job_postings.csv")
columns_to_drop = [
    "ID", "URL", "ACTIVE_URLS", "DUPLICATES", "LAST_UPDATED_TIMESTAMP",
    "NAICS2", "NAICS3", "NAICS4", "NAICS5", "NAICS6",
    "SOC_2", "SOC_3", "SOC_5"
]
lightcast_data.drop(columns=columns_to_drop, inplace=True)
```


# Which columns are irrelevant or redundant?

The removed columns include unique identifiers (such as ID), timestamps (LAST_UPDATED_TIMESTAMP), links (URL), duplicate markers (DUPLICATES), and overlydetailed industry and occupation classification codes (various levels of NAICS and SOC). These columns may have no direct impact on the analysis or be redundant, so they were removed to simplify data processing.

# Why are we removing multiple versions of NAICS/SOC codes?
The main reason for removing multiple versions of NAICS/SOC codes is to avoid data redundancy and confusion, ensuring data consistency and comparability. Different versions of NAICS may lead to inconsistencies in industry classification standards, which can affect the accuracy of data analysis.

# How will this improve analysis?
Removing multiple versions of NAIC/SOC codes can improve data consistency and comparability, preventing analysis inaccuracies caused by inconsistent industry classification standards. This helps reduce data redundancy, simplify data cleaning and processing, and minimize classification errors due to version differences,making the data more standardized and enhancing the quality and reliability of data analysis.

## **4.4 Handle Missing Values**
# How should we handle missing values?
1. Removing Missing Values (Dropping Data)
If missing values are rare and their removal won’t significantly impact the analysis, dropping them is a simple and effective solution. This can be done by removing rows containing missing values if they make up a small percentage of the dataset. Alternatively, if an entire column has a large proportion of missing values, it may be best to drop the column entirely. 
2. Filling Missing Values (Imputation)
Imputation involves replacing missing values with a calculated or default value. For numerical data, common methods include filling missing values with the median (to handle skewed distributions), the mean (if the data is normally distributed), or the mode (if there is a frequently occurring value). For categorical data, missing values can be replaced with "Unknown" to retain information or with the mode if a dominant category exists. This method preserves the dataset’s size and can prevent loss of valuable information.

```{python}
# Visualize missing data
msno.heatmap(lightcast_data)
plt.title("Missing Values Heatmap")
plt.show()
```

# identify numerical and categorical columns
```{python}
lightcast_data.head()
# Identify numerical columns
numerical_columns = lightcast_data.select_dtypes(include=['number']).columns

# Identify categorical columns
categorical_columns = lightcast_data.select_dtypes(include=['object', 'category']).columns
```

```{python}
# Find columns with missing values
missing_values = lightcast_data.isnull().sum()

# Filter only columns with missing data
missing_columns = missing_values[missing_values > 0]

# Display columns with missing values and their count
print(missing_columns)
```

# Numerical fields (e.g., Salary) are filled with the median.
```{python}
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning) 
# Iterate over columns with missing values
for col in missing_columns.index:
    if col in numerical_columns:
        # If numeric, replace missing values with the median
        lightcast_data[col].fillna(lightcast_data[col].median(), inplace=True)
    elif col in categorical_columns:
        # If categorical, replace missing values with "Unknown"
        lightcast_data[col].fillna("Unknown", inplace=True)

# Verify if missing values are handled
print(lightcast_data.isnull().sum())  # Should return 0 for all columns
```

# Drop columns with >50% missing values
```{python}
lightcast_data.dropna(thresh=len(lightcast_data) * 0.5, axis=1, inplace=True)
```

** # Remove Duplicates **
```{python}
lightcast_data = lightcast_data.drop_duplicates(subset=["TITLE", "COMPANY", "LOCATION", "POSTED"], keep="first")
```

```{python}
import plotly.express as px
from collections import Counter
import re

df = pd.DataFrame(lightcast_data)

skills_keywords = [
    "Python", "SQL", "Machine Learning", "Deep Learning", "AI", "TensorFlow", "PyTorch", 
    "Excel", "Tableau", "Power BI", "NLP", "Big Data", "Hadoop", "R", "Java", "C++"
]

skill_counts = Counter()
for desc in df["BODY"]:
    words = re.findall(r'\b\w+\b', desc)  # Extract words
    matched_skills = [word for word in words if word in skills_keywords]  # Filter keywords
    skill_counts.update(matched_skills)

# Convert to DataFrame
skills_df = pd.DataFrame(skill_counts.items(), columns=["Skill", "Count"]).sort_values(by="Count", ascending=False)

# Create bar chart using Plotly
fig = px.bar(
    skills_df, 
    x="Skill", 
    y="Count", 
    title="Most In-Demand Skills in Data Science and Analytics Jobs", 
    labels={"Skill": "Skills", "Count": "Frequency"}, 
    text=skills_df["Count"]
)

# Show the interactive chart
fig.show()
```


