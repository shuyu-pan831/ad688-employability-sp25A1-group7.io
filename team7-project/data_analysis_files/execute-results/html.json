{
  "hash": "d34872208748840a52e9f319d0a7049d",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Data Analysis\"\nsubtitle: \"Comprehensive Data Cleaning & Exploratory Analysis of Job Market Trends\"\nauthor:\n  - name: Norah Jones\n    affiliations:\n      - id: bu\n        name: Boston University\n        city: Boston\n        state: MA\n  - name: John Hamm\n    affiliations:\n      - ref: bu\nbibliography: /home/ubuntu/ad688-employability-sp25A1-group7.io/references.bib\ncsl: /home/ubuntu/ad688-employability-sp25A1-group7.io/csl/econometrica.csl\nformat: \n  html:\n    toc: true\n    number-sections: true\n    df-print: paged\n---\n\n::: {#ffce4b62 .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nlightcast_data = pd.read_csv(\"/home/ubuntu/ad688-employability-sp25A1-group7.io/team7-project/lightcast_job_postings.csv\")\ncolumns_to_drop = [\n    \"ID\", \"URL\", \"ACTIVE_URLS\", \"DUPLICATES\", \"LAST_UPDATED_TIMESTAMP\",\n    \"NAICS2\", \"NAICS3\", \"NAICS4\", \"NAICS5\", \"NAICS6\",\n    \"SOC_2\", \"SOC_3\", \"SOC_5\"\n]\nlightcast_data.drop(columns=columns_to_drop, inplace=True)\n```\n:::\n\n\n# Which columns are irrelevant or redundant?\n\nThe removed columns include unique identifiers (such as ID), timestamps (LAST_UPDATED_TIMESTAMP), links (URL), duplicate markers (DUPLICATES), and overlydetailed industry and occupation classification codes (various levels of NAICS and SOC). These columns may have no direct impact on the analysis or be redundant, so they were removed to simplify data processing.\n\n# Why are we removing multiple versions of NAICS/SOC codes?\nThe main reason for removing multiple versions of NAICS/SOC codes is to avoid data redundancy and confusion, ensuring data consistency and comparability. Different versions of NAICS may lead to inconsistencies in industry classification standards, which can affect the accuracy of data analysis.\n\n# How will this improve analysis?\nRemoving multiple versions of NAIC/SOC codes can improve data consistency and comparability, preventing analysis inaccuracies caused by inconsistent industry classification standards. This helps reduce data redundancy, simplify data cleaning and processing, and minimize classification errors due to version differences,making the data more standardized and enhancing the quality and reliability of data analysis.\n\n## **Handle Missing Values**\n\n\nimport missingno as msno\n\n# Visualize missing data\nmsno.heatmap(df)\nplt.title(\"Missing Values Heatmap\")\nplt.show()\n\n# Drop columns with >50% missing values\ndf.dropna(thresh=len(df) * 0.5, axis=1, inplace=True)\n\n# Fill missing values\ndf[\"Salary\"].fillna(df[\"Salary\"].median(), inplace=True)\ndf[\"Industry\"].fillna(\"Unknown\", inplace=True)\n```\n\n** #Remove Duplicates **\n\ndf = df.drop_duplicates(subset=[\"TITLE\", \"COMPANY\", \"LOCATION\", \"POSTED\"], keep=\"first\")\n\n",
    "supporting": [
      "data_analysis_files"
    ],
    "filters": [],
    "includes": {}
  }
}